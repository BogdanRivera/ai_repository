{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9d9376",
   "metadata": {},
   "source": [
    "# Ejercicio destilación: Maestro - Estudiante\n",
    "\n",
    "## Bogdan Kaleb García Rivera \n",
    "### MIA-2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac2ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Definir el modelo maestro y el modelo estudiante\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) #Aplanar la entrada\n",
    "        return self.fc(x)\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  #Aplanar la entrada\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9a3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar los modelos\n",
    "teacher_model = TeacherModel()\n",
    "student_model = StudentModel()\n",
    "\n",
    "# Definir la función de pérdida\n",
    "criterion_hard = nn.CrossEntropyLoss()\n",
    "criterion_soft = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "# Definir el optimizador\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f29aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función del entrenamiento del modelo estudiante\n",
    "def train(student_model, teacher_model, dataloader, alpha, temperature):\n",
    "    student_model.train()\n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generar etiquetas suaves con el modelo maestro\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model(data)\n",
    "            soft_target = torch.softmax(teacher_output / temperature, dim=1)\n",
    "\n",
    "        # Salidas del modelo estudiante\n",
    "        student_output = student_model(data)\n",
    "        hard_loss = criterion_hard(student_output, target)\n",
    "        soft_loss = criterion_soft(torch.log_softmax(student_output / temperature, dim=1), soft_target)\n",
    "\n",
    "        # Pérdida total\n",
    "        loss = alpha * hard_loss + (1 - alpha) * soft_loss\n",
    "\n",
    "        # Retropropagación y optimización\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Convertidor a tipo de dato apto para la red\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)), # Media y mediana de acuerdo a un estándar de un artículo\n",
    "    transforms.Lambda(lambda x: torch.flatten(x))  # Aplana a 784\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba7021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher(teacher_model, dataloader, epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\n",
    "    \n",
    "    teacher_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = teacher_model(data)  \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Épocas del maestro {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f19620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            output = model(data)  \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee786cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épocas del maestro 1, Loss: 0.3729\n",
      "Épocas del maestro 2, Loss: 0.5040\n",
      "Épocas del maestro 3, Loss: 0.1402\n",
      "Épocas del maestro 4, Loss: 0.1446\n",
      "Épocas del maestro 5, Loss: 0.3956\n",
      "Precisión del maestro antes de la destilación: 92.01%\n",
      "Precisión del estudiante antes de la destilación: 16.27%\n",
      "Precisión del estudiante después de la destilación: 91.79%\n"
     ]
    }
   ],
   "source": [
    "# Descargar y cargar el conjunto de datos MNIST de entrenamiento\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "# Carga de datos de entrenamiento \n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Datos de prueba\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Entrenar al maestro\n",
    "train_teacher(teacher_model, train_loader, epochs=5)\n",
    "\n",
    "# Evaluación del modelo antes de la destilación\n",
    "teacher_eval = evaluate(teacher_model, test_loader)\n",
    "student_eval = evaluate(student_model, test_loader)\n",
    "print(f\"Precisión del maestro antes de la destilación: {teacher_eval:.2f}%\")\n",
    "print(f\"Precisión del estudiante antes de la destilación: {student_eval:.2f}%\")\n",
    "\n",
    "# Entrenar el modelo del estudiante\n",
    "train(student_model, teacher_model, train_loader, alpha=0.5, temperature=2.0)\n",
    "\n",
    "# Evaluación del estudiante después de la destilación \n",
    "student_acc_after = evaluate(student_model, test_loader)\n",
    "print(f\"Precisión del estudiante después de la destilación: {student_acc_after:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
